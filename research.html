---
layout: page
title: Research
---

<h2 style="text-align:center">Publications</h2>
<hr style="height:1px;border:none;color:#333;background-color:#333;" />
<h3 style="text-align:center">HOnnotate: A method for 3D Annotation of Hand and Object Poses</h3>
<h4 style="text-align:center">
 <a href="http://shreyashampali.github.io/"> Shreyas Hampali,</a>
 <a href="https://www.tugraz.at/institute/icg/research/team-lepetit/people/mahdi-rad/"> Mahdi Rad,</a>
 <a href="https://www.tugraz.at/institute/icg/research/team-lepetit/people/markus-oberweger/"> Markus Oberweger,</a>
 <a href="https://www.labri.fr/perso/vlepetit/"> Vincent Lepetit</a>
</h4>
<h4 style="text-align:center"><i>In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2020</i></h4>
<p style="text-align:center;"><img src="/img/honnotate.png" alt="HOnnotate" width="1258"height="300"</p>
<p><b>Abstract</b>: We propose a method for annotating images of a hand manipulating an object with the 3D poses of both the hand and the object, together with a dataset created using this method. There is a current lack of annotated real images for this problem, as estimating the 3D poses is challenging, mostly because of the mutual occlusions between the hand and the object. To tackle this challenge, we capture sequences with one or several RGB-D cameras, and jointly optimizes the 3D hand and object poses over all the frames simultaneously. This method allows us to automatically annotate each frame with accurate estimates of the poses, despite large mutual occlusions. With this method, we created HO-3D, the first markerless dataset of color images with 3D annotations of both hand and object. This dataset is currently made of 80,000 frames, 65 sequences, 10 persons, and 10 objects. We also use it to train a deepnet to perform RGB-based single frame hand pose estimation and provide a baseline on our dataset.</p>
<p>[<a href="https://arxiv.org/abs/1907.01481">paper</a>] [<a href="https://drive.google.com/file/d/1FmJjSnr00IfUWRiJG_LVCpQUJETjmMY4/view?usp=sharing"> Supplementary</a>] [<a href="https://github.com/shreyashampali/HOnnotate"> Code</a>]</p>
<p><b>Dataset: </b>Please visit the <a href="https://www.tugraz.at/index.php?id=40231">project page</a> to download the dataset</p>
<p><b>CodaLab Competition: </b>Online challenge for hand pose estimation from single RGB image on our dataset <a href="https://competitions.codalab.org/competitions/22485?">here</a></p>
<p><b>Video:</b></p>
<p style="text-align:center;">
<iframe width="700" height="395" src="https://www.youtube.com/embed/S1acEA0U0hk?rel=0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</p>
<hr style="height:1px;border:none;color:#333;background-color:#333;" />
<h3 style="text-align:center">General 3D Room Layout from a Single View by Render-and-Compare</h3>
<h4 style="text-align:center">
 <a href="https://www.tugraz.at/institute/icg/research/team-lepetit/people/sinisa-stekovic/"> Sinisa Stekovic,</a>
 <a href="http://shreyashampali.github.io/"> Shreyas Hampali,</a>
 <a href="https://www.tugraz.at/institute/icg/research/team-lepetit/people/mahdi-rad/"> Mahdi Rad,</a>
 <a href="https://www.tugraz.at/institute/icg/research/team-lepetit/people/sayan-deb-sarkar/"> Sayan Deb Sarkar,</a>
 <a href="https://www.tugraz.at/institutes/icg/research/team-fraundorfer/people/friedrich-fraundorfer/"> Friedrich Fraundorfer</a>
 <a href="https://www.labri.fr/perso/vlepetit/"> Vincent Lepetit</a>
</h4>
<h4 style="text-align:center"><i>In Proc. European Conference on Computer Vision (ECCV), 2020</i></h4>
<p><b>Abstract</b>: We present a novel method to reconstruct the 3D layout of
a room—walls, floors, ceilings—from a single perspective view in challenging conditions, by contrast with previous single-view methods restricted to cuboid-shaped layouts. This input view can consist of a color
image only, but considering a depth map results in a more accurate
reconstruction. Our approach is formalized as solving a constrained discrete optimization problem to find the set of 3D polygons that constitute
the layout. In order to deal with occlusions between components of the
layout, which is a problem ignored by previous works, we introduce an
analysis-by-synthesis method to iteratively refine the 3D layout estimate.
As no dataset was available to evaluate our method quantitatively, we
created one together with several appropriate metrics. Our dataset consists of 293 images from ScanNet, which we annotated with precise 3D
layouts. It offers three times more samples than the popular NYUv2 303
benchmark, and a much larger variety of layouts.</p>

<p>[<a href="https://arxiv.org/pdf/2001.02149.pdf">paper</a>] [<a href="https://files.icg.tugraz.at/f/4ebd05b88f0d4e00be07/?dl=1"> Supplementary</a>] [<a href="https://github.com/vevenom/ScanNet-Layout"> ScanNet-Layout Dataset</a>] [<a href="https://github.com/vevenom/RoomLayout3D_RandC"> Code</a>]</p>
<hr style="height:1px;border:none;color:#333;background-color:#333;" />

<h3 style="text-align:center">Measuring Generalisation to Unseen Viewpoints,
Articulations, Shapes and Objects for 3D Hand
Pose Estimation under Hand-Object Interaction</h3>
<h4 style="text-align:center">
 Anil Armagan, Guillermo Garcia-Hernando, Seungryul Baek, Shreyas Hampali, ..., Vincent Lepetit
</h4>
<h4 style="text-align:center"><i>In Proc. European Conference on Computer Vision (ECCV), 2020</i></h4>
<p><b>Abstract</b>: We study how well different types of approaches generalise in
the task of 3D hand pose estimation under single hand scenarios and handobject interaction. We show that the accuracy of state-of-the-art methods
can drop, and that they fail mostly on poses absent from the training set.
Unfortunately, since the space of hand poses is highly dimensional, it is
inherently not feasible to cover the whole space densely, despite recent
efforts in collecting large-scale training datasets. This sampling problem
is even more severe when hands are interacting with objects and/or
inputs are RGB rather than depth images, as RGB images also vary with
lighting conditions and colors. To address these issues, we designed a
public challenge (HANDS’19) to evaluate the abilities of current 3D hand
pose estimators (HPEs) to interpolate and extrapolate the poses of a
training set. More exactly, HANDS’19 is designed (a) to evaluate the
influence of both depth and color modalities on 3D hand pose estimation,
under the presence or absence of objects; (b) to assess the generalisation
abilities w.r.t. four main axes: shapes, articulations, viewpoints, and
objects; (c) to explore the use of a synthetic hand models to fill the
gaps of current datasets. Through the challenge, the overall accuracy has
dramatically improved over the baseline, especially on extrapolation tasks,
from 27mm to 13mm mean joint error. Our analyses highlight the impacts
of: Data pre-processing, ensemble approaches, the use of a parametric 3D
hand model (MANO), and different HPE methods/backbones.</p>
<p>[<a href="https://sites.google.com/view/hands2019/challenge">Challenge website</a>] </p>
<hr style="height:1px;border:none;color:#333;background-color:#333;" />

<h2 style="text-align:center">Patents</h2>
<hr style="height:1px;border:none;color:#333;background-color:#333;" />
<ol>
 <li><b>Shreyas Hampali</b>, “Video Coding Including a Stage-Interdependent Multi-Stage
Butterfly Integer Transform”, U.S. Patent 20160021369, published Jan 21, 2016.</li>
  <li><b>Shreyas Hampali</b>, Ajit Rao, Yogesh Gupta and Conrad Harrison, “Artifact detection
in a contrast enhanced output image”, U.S. Patent filed, Application no. 15/702,394</li>
 <li><b>Shreyas Hampali</b>, Pawan Baheti and Naveen Srinivasamurthy, “Systems and methods for non-recursive image signal processor tuning using a reference
image”, India Patent filed, Application no. 201841003400</li>
 <li>Shilpi Sahi, Pawan Baheti, Aarrushi Shandilya, <b>Shreyas Hampali</b>,
Naveen Srinivasamurthy and Yogesh Gupta, “Systems and methods for assisted image signal processor tuning”, India Patent filed, Application no. 201841003395</li>
 <li>Pawan Baheti, Shilpi Sahu, Naveen Srinivasamurthy, Yogesh Gupta, Uday Kiran
Pudipeddi, <b>Shreyas Hampali</b>, “Systems and methods for assisted
image signal processor tuning using a reference image”, India patent filed, Application no. 201841003373</li>
 <li><b>Shreyas Hampali</b> and Dowray Raghvendra Rao, ”Remote Image based Measurement
System”, India Patent 4785/CHE/2012, filed November 2012.</li>
</ol>
<hr style="height:1px;border:none;color:#333;background-color:#333;" />

